#data configs

#tokenize
#input_jsonl: ../../data/text/corpus_scale_200.jsonl
#output_prefix: ../../data/text/0420test

#train
#tokenized_data_path: /storage5/split/btm_ja/BTM_J
tokenized_data_path: /storage5/text_corpus/phase2_jap_en_tokenized/0601_phase1_corpus/tokenized_text/corpus_text_document
megatron_deepspeed_dir: Megatron-DeepSpeed
input_tokenizer_file: /storage5/text_corpus/phase2_jap_en_tokenized/0529tokenizer_65k_wo_mecab_scale50.model
output_model_dir: /storage5/50b_checkpoints
#save_interval: 1500
#save_interval: 700
save_interval: 350 #2024/6/13修正

#save_interval: 10 #global batchに応じて変える
#save_interval: 1000

#llama70b
#hidden 8192
#atten heads 64
#layers 80


#llama3 50b?
#model_size: 46
#num_layers: 85

model_size: 39
num_layers: 72



hidden_size: 6912
ffn_hidden_size: 16128 # hiddensizeの4.5倍にすること
#ffn_hidden_size: 24576 # hiddensizeの4倍にしたパターン
#num_attn_heads: 48
num_attn_heads: 54

lr: 2.0e-4
#min_lr: 2.0e-6
min_lr: 1.99e-4
init_std: 0.02
NHOSTS: 18
#NHOSTS: 1

mp_size: 1
pp_size: 6
#pp_size: 1 # 1 nodeのときは１にする
sp_size: 1

global_batch_size: 2304
micro_batch_size: 1
#token info
#train_tokens: 144112147
train_tokens: 175408565764
lr_decay_tokens_in_billion: 175
lr_decay_tokens: 145408565764
#この値をきちんと決めておかないと、learning rateのスケジュラーがきちんと動かない
train_samples: 300000000 # 300B相当｡ もっと学習させる場合は大きくすること!!

seq_len: 2048

#train_samples=$(( 300 * 1000 * 1000 * 1000 * 2 / ${seq_len} ))

zero_stage: 1
num_workers: 200

activation_checkpoint: true